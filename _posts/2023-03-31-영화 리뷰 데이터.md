```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

### 문제 정의
- 영화 리뷰 데이터셋을 활용해 긍정/부정 리뷰를 구분하는 감성분석을 해보자!!
- 긍정/부정 리뷰에서 자주 사용되는 단어를 확인해보자!!

### 데이터 수집
- Large movie dataset 다운로드


```python
from sklearn.datasets import load_files
```


```python
train_data_url = 'aclImdb/train/'
reviews_train = load_files(train_data_url,shuffle=True)
```


```python
test_data_url = 'aclImdb/test/'
reviews_test = load_files(test_data_url,shuffle=True)
```


```python
reviews_train.keys()
```


```python
reviews_train.data
```


```python
# 리뷰의 갯수(긍정/부정)
len(reviews_train.data)
```


```python
len(reviews_train.data)
```


```python
reviews_test.target
```

### 데이터 전처리
- 일반 정형 데이터 : 결측치, 스케일링, 이상치, 특성공학...
- 텍스트 데이터(비정형 데이터)
    - 오탈자 제거
    - 띄어쓰기 교정
    - 이모티콘 수정
    - 불필요한 글자 제거
    - 데이터 정형화(토큰화, 수치화)
        - 토큰화 : 문장을 잘게 쪼개줌, 수치화 : 숫자로 표현


```python
reviews_train.data
```


```python
# 한 줄 for문(여러 줄로 작성해도 상관없다.)
# replace : 문자열 함수(빈칸으로 수정하면 삭제시키는 효과)
# b : 데이터 타입을 의미함(바이너리)
text_train = [txt.replace(b'<br />', b' ') for txt in reviews_train.data]
```


```python
text_train
```


```python
text_test = [txt.replace(b'<br />', b' ') for txt in reviews_test.data]
```


```python
text_test
```

### BOW(Bag of word)
- 단어 토큰화 및 단어사전 구축


```python
from sklearn.feature_extraction.text import  CountVectorizer
```


```python
movie_count = CountVectorizer()
```


```python
# 토큰화 + 단어사전 구축
movie_count.fit(text_train)
```


```python
len(movie_count.vocabulary_)
```


```python
# 딕셔너리 타입
movie_count.vocabulary_
```

- 수치 데이터로 변경


```python
X_train = movie_count.transform(text_train)
X_train
```


```python
X_test = movie_count.transform(text_test)
X_test
```

#### 탐색적 데이터 분석(시각화) 생랴4

### 모델 선택


```python
from sklearn.linear_model import LogisticRegression
# 선형 모델 - 고차원 데이터에서 성능이 괜찮게 나오는 편!!
```


```python
logi_model = LogisticRegression()
```


```python
from sklearn.model_selection import cross_val_score
```


```python
logi_result = cross_val_score(logi_model,X_train,reviews_train.target, cv=3)
```


```python
logi_result.mean()
```


```python
from sklearn.svm import LinearSVC
```


```python
svc_model = LinearSVC()
```


```python
svc_result = cross_val_score(svc_model,X_train,reviews_train.target, cv=3)
```


```python
svc_result.mean()
```

### 모델 하이퍼파라미터 튜닝
- CountVectorizer
    - min__df : 최소문장 빈도수(전체 문서 중 등장해야하는 최소한의 빈도수 설정, ex)x번이상은 나와야 사전에 등록가능하다.)
    - max__df : 최대문장 빈도수(전체 문서 중 등장해야하는 최대한의 빈도수 설정, ex)x번이상나와야 별 의미가없다.)
    - n_gram : 토큰을 묶는 숫자를 설정(유니그램, 바이그램, 트라이그램)
- LoggisticRegression
    - C(규제) : 선형 모델의 가중치를 규제하는 정도를 설정함!

### pipe line


```python
from sklearn.pipeline import make_pipeline
# 지금까지는 단일 모델만 살펴봤는데, (하나만 볼 수 없음!)
# 지금은 count, logi 둘 다 봐야 좋은 성능 모델링을 할 수 있다!
# 세트로 묶어주자 -> pipeline으로 묶어줌
```


```python
pipe_model = make_pipeline(CountVectorizer(),LogisticRegression())
```


```python
from sklearn.model_selection import GridSearchCV
```


```python
params={
    "countvectorizer__max_df" : [15000,20000,23000],
    "countvectorizer__min_df" : [3,5,7],
    "countvectorizer__ngram_range" : [(1,1),(1,2),(1,3)],
    "logisticregression__C" : [0.01, 0.1, 1, 10, 100]
}
```


```python
grid = GridSearchCV(pipe_model, params, cv=3)
```


```python
grid.fit(text_train, reviews_train.target)
```


```python
grid.best_score_
```


```python
grid.best_params_
```


```python
best_model = grid.best_estimator_
```

### 모델 평가


```python
best_model.score(text_test, reviews_test.target)
```

### 모델 활용
- 로튼토마토에서 실제 댓글/리뷰 가지고옴!!


```python
review = ["Awesomeness...yep never bet against Cameron! The only true epic director next to Ridley Scott"]
```


```python
best_model.predict(review)
```


```python
review2=["I didnt the like the first one the story was bland..."]
best_model.predict(review2)
```


```python
review3 = ["From the very beginning “Avengers: Endgame” feels like something special, something unique, something unlike anything we’ve seen before. And even in its missteps it never loses that sense of spectacle and grandeur."]
best_model.predict(review3)
```

### 학습한 모델을 통해 단어별 가중치를 확인해보자!


```python
best_model
```


```python
best_model[0]
```


```python
best_model[1]
```


```python
best_logi_model = best_model[1]
# 가중치!!는 모델이 가지고 있음
best_logi_model.coef_
```


```python
best_logi_model.coef_.shape
```


```python
# 단어사전은 countvectorizer가 가지고 있음
count_model = best_model[0]
count_model.vocabulary_
```


```python
len(count_model.vocabulary_)
```


```python
# DataFrame 만들기
df=pd.DataFrame([count_model.vocabulary_.keys(),
             count_model.vocabulary_.values()])
df
```


```python
# 알파벳 순서대로 정렬
df=df.T.sort_values(by=1)
df
```


```python
df
```


```python
best_logi_model.coef_.reshape(-1)
```


```python
best_logi_model.coef_.reshape(-1).shape
```


```python
# "가중치" 컬럼 생성!
df["coef"]=best_logi_model.coef_.reshape(-1)
```


```python
# 가중치 기준으로 정렬!
df.sort_values(by="coef", inplace=True)
```


```python
# 부정에 가장 영향을 많이 미치는 단어 30개
df.head(30)[[0,"coef"]]
```


```python
# 긍정에 가장 영향을 많이 미치는 단어 30개
df.tail(30)[[0,"coef"]]
```


```python
pos_neg_top_30 = pd.concat([
    df.head(30)[[0,"coef"]],
    df.tail(30)[[0,"coef"]]
])
```


```python
pos_neg_top_30
```


```python
pos_neg_top_30[0]
```


```python
plt.figure(figsize=(15,5))
plt.bar(pos_neg_top_30[0], pos_neg_top_30["coef"])
plt.xticks(rotation=90)
plt.show()
```
